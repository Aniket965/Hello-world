import pandas as pd
import quandl, math
import numpy as np
from statistics import mean

class LinearRegression:
    
    #initializing the parameters required by the model
    def __init__(self, alpha=0.01, fit_intercept=True, verbose=False):
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.verbose = verbose
    
    #hypothesis function
    def __calculate_h(self, X_train):
        return np.dot(X_train,self.theta)
   
    #cost function
    def __calculate_j(self, h_theta, Y_train):
        return (((h_theta - Y_train)**2).mean())/2
    
    #adding intercept to the training set
    def __add_intercept(self, X_train):
            intercept = np.ones((X_train.shape[0], 1))
            return np.concatenate((intercept, X_train), axis=1)
     
    #main algorithm, which uses gradient descent at the hood
    def fit(self, X_train, Y_train):
        
        #adding intercept
        if self.fit_intercept:
            X_train = self.__add_intercept(X_train)
        
        #initializing theta
        self.theta = np.zeros(X_train.shape[1])
        
        #training set size
        m = Y_train.size
        
        iter_cost = []
        
        h_theta =  self.__calculate_h(X_train)
        j_theta = self.__calculate_j(h_theta, Y_train)
        prev_j_theta = j_theta
        iteration = 1
        while 1:
                        
            gradient = np.dot(X_train.T, (h_theta - Y_train)) / m
            self.theta -= self.alpha * gradient
            
            h_theta =  self.__calculate_h(X_train)
            j_theta = self.__calculate_j(h_theta, Y_train)
            iter_cost.append([iteration, j_theta])
            
            if self.verbose and iteration%10 == 0:
                print('Iter. No : ' + str(iteration) + ' -> Cost : ' + str(j_theta))                
            
            iteration+=1
            if prev_j_theta - j_theta < 0.001:
                break
            prev_j_theta = j_theta
            
        return iter_cost
    
    #predicts the value for the given test set
    def predict(self, X_test):
        
        if self.fit_intercept:
            X_test = self.__add_intercept(X_test)
        
        return self.__calculate_h(X_test)
    
    def __squared_error(self, ys_orig, ys_line):
        return sum((ys_line - ys_orig)**2)
    
     #function which calculates accuracy of the trained model
    def accuracy(self, X_test, Y_test):
        Y_pred = self.predict(X_test)
        y_mean_line = [mean(Y_test) for y in Y_test]
        squared_error_regr = self.__squared_error(Y_test, Y_pred)
        squared_error_y_mean = self.__squared_error(Y_test, y_mean_line)
        return 1 - (squared_error_regr/squared_error_y_mean)

