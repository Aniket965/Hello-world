{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "**data:** It is basically the dataset, collection of text in a file cleaned and processed also known as text corpus.\n",
    "\n",
    "**ix_to_char:** Dictionary mapping, indexes(ix) to their respective characters(char), it is done so as to identify the character, like we do in a book's index.\n",
    "\n",
    "**char_to_ix:** Dictionary mapping, characters(char) to their respective indexes(ix), it is done so as to identify which index is related to which character, so that we get our output as characters, use to convert indexes into characters.\n",
    "\n",
    "**num_iterations:** Iterations you want your model to go through.\n",
    "\n",
    "**n_a:** It is the number of RNN/LSTM cells you want your model to have.\n",
    "\n",
    "**sample_size:** It is the number of sample, & is used here to get sample of outputs after certain number of iterations to see how the model is doing.\n",
    "\n",
    "**vocab_size:** It is the number of unique items in the data set, example if you are making character based RNN your vocab size will be number of unique albhabets and characters( here 26 alphabets + 1 EOF('\\n') ) & if you are making word based RNN your vocab size will be number of unique words(that can very huge).\n",
    "\n",
    "****THERE ARE SOME IMPORTANT TERMS I HAVE DEFINED IN THE FUNCTIONS INSTEAD, TO GET A BETTER IDEA**\n",
    "**PLEASE SEARCH FOR:**\n",
    "**TIME_STEPS in INITIALIZE_PARAMETERS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll commenting below the code lines.\n",
    "\n",
    "def RNN( data, ix_to_char, char_to_ix, num_iterations = 1000000, n_a = 50, sample_size=7, vocab_size=27 ):\n",
    "    \n",
    "    \n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    # n_x--> This is the size for a one-dimensional vector of 0's and 1's known as one-hot encoded vector.\n",
    "    #        In our model instead of indexes, we provide these one-hot encoded vectors, to\n",
    "    #        represent the characters.\n",
    "    # Why we use one-hot encoded vectors?\n",
    "    # This binary representation does not make any assumptions about similarity of data points:\n",
    "    # they are either equal or not.\n",
    "    # say if we have \"a\" indexed at 1,\n",
    "    #                \"b\" indexed at 2,\n",
    "    #                \"c\" indexed at 3,\n",
    "    #In this sense our model may get misguided that c=a+b, \n",
    "    #i.e. mathematical relations may misguide logical relations.\n",
    "    \n",
    "    #As here output will also be a character, therefore n_x= n_y= 27.\n",
    "    \n",
    "    parameters = initialize_parameters( n_a, n_x, n_y )\n",
    "    # To randomly initialize weights and biases for different connections.\n",
    "    \n",
    "    loss = initial_loss( vocab_size, sample_size )\n",
    "    # For smoothing of loss/ ignore this if aren't familiar with it.\n",
    "    \n",
    "    with open( \"data_file\" ) as file:\n",
    "        examples = file.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    # Making a python list of different example from the text corpus.\n",
    "    \n",
    "    np.random.shuffle(examples)\n",
    "    # Shuffling of the examples, to remove the possibility of model learning on the basis of\n",
    "    # sequence of the data.\n",
    "    # We want our model to be independent of everyother pattern except the text corpus.\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    # Initializing hidden weigth matrix, with zeros.\n",
    "    \n",
    "    #---------------------------------The iteration/optimization loop----------------------------------------#\n",
    "    for j in range(num_iterations):\n",
    "    # Iterating the model several times so as to get our parameters induced with every possible\n",
    "    # approximation of the function which we are trying to build by reducing cost-function(loss). \n",
    "    \n",
    "        #----------------------making of input X and prediction Y------------------#\n",
    "        index = j % len(examples)\n",
    "        # Starting from 0th example to the last example, as  0<=index<len(example).\n",
    "        \n",
    "        X = [None] +[char_to_ix[ch] for ch in examples[index]]\n",
    "        # (X-->input)Converting our example from examples[] into a python list of character's index.  \n",
    "        \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        # (Y-->predictions to be made) shifting X to the left and adding \"\\n\"'s index.\n",
    "        # If you are not familiar with python, then google \"list slicing\".\n",
    "        \n",
    "        #example  X=[\"None\",\"A\",\"P\",\"P\",\"L\",\"E\"]\n",
    "        #         Y=[\"A\",\"P\",\"P\",\"L\",\"E\",\"\\n\"],  if model get \"None\" it should predict \"A\", and so on.\n",
    "        #In above exapmle I have shown X & Y a list characters but instead it will be a list of their indices.\n",
    "        # It is just for representational/understanding purposes.\n",
    "\n",
    "        #----------------------end of input/prediction making---------------------#\n",
    "        \n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        # One step of Forward prop--> Backward prop--> Gardient-clipping--> Update parameters.\n",
    "        # We will fetch current loss value, dictionary of various gradients, & the hidden layer.\n",
    "        # The function and \"a_prev\" will be explained thoroughly later through this code.\n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        # Ignore for now.\n",
    "        \n",
    "        #---------------------------------sampling-----------------------------------#\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            for name in range(sample_size):\n",
    "                \n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                # We are performing sampling here, will be expalined thoroughly later.\n",
    "                # Just remeber we do it to keep a check on our outputs.\n",
    "                # so, by this we will be predicting a number of ouputs, \n",
    "                # and seeing how the model is performing.\n",
    "                \n",
    "                print_sample(sampled_indices, ix_to_char) \n",
    "                # Printing the sample predictions\n",
    "            print('\\n')\n",
    "        #-------------------------------end of sampling-------------------------------#\n",
    "    \n",
    "    #---------------------------------end of iteration/optimization loop-----------------------------------#\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 \n",
    "    # input to hidden weight matrix\n",
    "    # dimensions--> ( number of RNN units, size of the one-hot encoded vector to represent a single character)\n",
    "#-----------------------------------------------------------------------------------------------------------------#\n",
    "    # Here I want to take some time and brief upon THE USE OF MATRICES IN CONTEXT OF RNNs:\n",
    "    \n",
    "    # We use term \"time steps\" in RNN which is basically \n",
    "    #(here) The position of a character in our single example \n",
    "    # e.g. [\"A\",\"P\",\"P\",\"L\",\"E\",\"/n\"]\n",
    "    # let we are at position \"P\"\n",
    "    # Then the characters after(\"P\",\"L\",\"E\",\"/n\") it are regarded as in future time-step.\n",
    "    # & the character before(\"A\") it is regarded as past time-step. \n",
    "    \n",
    "    # Matrices are used to spare us from the loops to move through time-steps.\n",
    "    # **Moving through time-steps just means how many working RNN units/cells we have\n",
    "    # So, what we do(here/above) instead of for loops is to create a (n_a, n_x) dimensional matrix\n",
    "    # which simultaneuosly multiply Weight matrix with our inputs, through all the time-steps.\n",
    "    # which is like storing the whole word character by character, but at the same time.\n",
    "    # philosophically we are dealing with future and past in present.\n",
    "#-----------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 \n",
    "    # hidden to hidden weight matrix\n",
    "    #dimensions--> ( number of RNN units, number of RNN units)\n",
    "    \n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 \n",
    "    # hidden to output weight matrix\n",
    "    #dimensions--> ( one-hot encoded output vector for a single character, number of RNN units)\n",
    "    \n",
    "    b = np.zeros((n_a, 1)) \n",
    "    # hidden bias matrix\n",
    "    # dimensions--> ( number of RNN units, 1)\n",
    "    # these are just column vectors, which broadcasts(a python terminology) them during addition.\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------#\n",
    "    # Here I want to take some time and brief upon BIAS and VARIANCE:\n",
    "    \n",
    "    # BIAS error arises while:\n",
    "    # Approximating a problem, which may be extremely complicated, by a much simpler model.\n",
    "    # That means if the model doesn't even fit your training set.\n",
    "    # making your model more complex can solve this problem.\n",
    "    \n",
    "    # VARIANCE error:\n",
    "    # When we train on different set, our model approximates the functions accordingly,\n",
    "    # so how much the approximation varies on training on different set, is determined by the VARIANCE.\n",
    "    # high variance means overfitting.\n",
    "    # to solve this problem try regularization, or more data.\n",
    "    \n",
    "    #TO SUM UP:\n",
    "    # As a statistical method tries to match data points more closely or when a more flexible method is used, \n",
    "    # the bias reduces, but variance increases.\n",
    "#-----------------------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "    by = np.zeros((n_y, 1)) \n",
    "    # output bias matrix\n",
    "     \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    # saving the inititalized weights in parameters dictionary.\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is made to combine:\n",
    "\n",
    "# (1.) forward propagation --> loss calculation\n",
    "\n",
    "# (2.) backward propagation --> gradient calculations:\n",
    "# i.e. derivative of loss functions with respect to parameters,\n",
    "# derivatives are applied using chain rule, \n",
    "# so that we can effect the weight matrix of one connection due to the change in some other connection's matrix.\n",
    "\n",
    "# (3.) gradient clipping --> to avoid exploding gradient problem which gives us \"NaN\"(not a number).\n",
    "\n",
    "# (4.) update parameters --> to update the weight matrix with new learned derivativespy.\n",
    "\n",
    "def optimize( X, Y, a_prev, parameters, learning_rate = 0.01 ):\n",
    "    \n",
    "    loss, cache = forward_prop( X, Y, a_prev, parameters )\n",
    "    #1\n",
    "    \n",
    "    gradients, a = backward_prop(X, Y, parameters, cache)\n",
    "    #2\n",
    "    \n",
    "    gradients = clip(gradients, 5)\n",
    "    #3\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    #4\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n",
    "    # We return :\n",
    "    # loss,\n",
    "    # clipped and updated gradients,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, Y, a_0, parameters, vocab_size = 27):\n",
    "    \n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    # Here we initializes three dictionaries:\n",
    "    # x--> one-hot encoded character dictionary for one example passed to this function.\n",
    "    # a--> activation laeyrs' dictionary.\n",
    "    # y_hat--> the softmax probabilities, of the predicted character.\n",
    "    \n",
    "    a[-1] = np.copy(a_0)\n",
    "    # saving a copy of a_0(the starting activation often a zero vector)\n",
    "    \n",
    "    loss = 0\n",
    "    # initializing the loss to zero\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "    # looping for one example we got e.g. X=[\"None\",\"A\",\"P\",\"P\",\"L\",\"E\"] len(X)=6 \n",
    "    # P.S. X is not a list of characters but their indices\n",
    "    # It is just for representational purposes.\n",
    "    \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        # first t'th key is set to zero vector. \n",
    "        \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        # and if it is not None then the X[t] position in the zero vector is set to 1.\n",
    "        # Why X[t]'th position:\n",
    "        # as in char_to_ix dictionary\n",
    "        # The position of charcaters represent their value\n",
    "        \n",
    "        #example:\n",
    "        # X=[\"0\",\"1\",\"16\",\"16\",\"12\",\"5\"] which is basically [\"None\",\"A\",\"P\",\"P\",\"L\",\"E\"]\n",
    "        # so x[0]=[1,0,0,0,...,0] (27 items)\n",
    "        # x[1] = [0,1,0,0,...,0] (27 items )\n",
    "        # x[2] = [0,0,0,0,...,1,...,0] (27 items)\n",
    "        \n",
    "        a[t], y_hat[t] = rnn_cell(parameters, a[t-1], x[t])\n",
    "        # fetching present activation functions and predictions \n",
    "        # via single RNN unit\n",
    "        \n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "    \n",
    "    cache = (y_hat, a, x)\n",
    "    #storing all these in cache\n",
    "    \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) \n",
    "    # hidden state\n",
    "    # as x is a column matrix of dimensions discussed earlier\n",
    "    # we can dot prodcut it\n",
    "    \n",
    "    y = np.dot(Wya, a_next) + by\n",
    "    # Ouptput value matrix\n",
    "    \n",
    "    p_t = softmax(y) \n",
    "    # Unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    \n",
    "    e_x = np.exp(y - np.max(y))\n",
    "    \n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue,out=gradient)\n",
    "  \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(X, Y, parameters, cache):\n",
    "    \n",
    "    \n",
    "    gradients = {}\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    \n",
    "    \n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    # Retrieve from cache and parameters\n",
    "    \n",
    "    \n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    \n",
    "    \n",
    "    for t in reversed(range(len(X))):\n",
    "    # Backpropagate through time\n",
    "    \n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = grad_calc(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "   \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_calc(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    \n",
    "    gradients['dby'] += dy\n",
    "    \n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] \n",
    "    # backprop into h\n",
    "    \n",
    "    daraw = (1 - a * a) * da \n",
    "    # backprop through tanh nonlinearity\n",
    "    \n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling for ouputting the predictions\n",
    "\n",
    "def sample(parameters, char_to_ix):\n",
    "     \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    \n",
    "    vocab_size = by.shape[0]\n",
    "    #fetching vocab_size\n",
    "    \n",
    "    n_a = Waa.shape[1]\n",
    "    #fetching number of rnn cell\n",
    "    \n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # making one-hot encoded vector\n",
    "    \n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    #making activation vector\n",
    "    \n",
    "    indices = []\n",
    "    # Create an empty list of indices, \n",
    "    # this is the list which will contain the list of indices of the characters to generate \n",
    "    \n",
    "    idx = -1 \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "   \n",
    "    \n",
    "    counter = 0\n",
    "    #initializing counter for number of characters to be predcited\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    #index of '\\n'\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "    #either print atleast 50 chars or stop on '\\n'\n",
    "    \n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        # activation matrix getting made via learned parameters Wax,Waa & b\n",
    "        \n",
    "        z = np.dot(Wya, a) + by\n",
    "        # output matrix getting made via learned parameters Wya and by\n",
    "        \n",
    "        y = softmax(z)\n",
    "        # softmax probailities for char prediction\n",
    "        \n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        \n",
    "        indices.append(idx)\n",
    "        #appending the index into indices list\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        # Overwrite the input character .\n",
    "        \n",
    "        x[idx] = 1\n",
    "        # setting 1 corresponding to the sampled index.\n",
    "        \n",
    "        a_prev = a\n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        \n",
    "        counter+=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(indices, ix_to_char):\n",
    "    \n",
    "    txt = ''.join(ix_to_char[ix] for ix in indices)\n",
    "    \n",
    "    txt = txt[0].upper() + txt[1:]  \n",
    "    # capitalize first character \n",
    "    \n",
    "    print ('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data_file', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = RNN(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moramery\n",
      "Riellin\n",
      "Pargele\n",
      "Zobrra\n",
      "Kantry\n",
      "Forta\n",
      "Rryrini\n",
      "Burde\n",
      "Chriethei\n",
      "Gurill\n"
     ]
    }
   ],
   "source": [
    "#after training the results are :\n",
    "for i in  range(10):\n",
    "    indices = sample(parameters, char_to_ix)\n",
    "    print_sample(indices, ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nice names bye"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
