{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\"><bold>RESULT PREDICTOR FOR BOXING MATHCES by DNN</bold></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">\n",
    "<ul>\n",
    "<li>This is a walk through a deep neural network which predicts outcome of a boxing match with no use of frameworks.</li>\n",
    "<li>Prerequisites are understanding of all basic terms. </li>\n",
    "<li>I have tried to explain every term as low as I could.</li>\n",
    "<li>Even if you don't understand it at a first glance, don't worry.</li>\n",
    "<li>I have not explain some term immediately as they are used, they must have been explained somewhere later.</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">\n",
    "</p> Functions for activations</p>\n",
    "</font>\n",
    "<font size=\"+1\">\n",
    "<ul>\n",
    "<li>These are used as a threshold measure, like the way neurons in your brain fires after a certain thresholds. </li>\n",
    "    <li>Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A Neural Network without Activation function would simply be a Linear regression Model</li>\n",
    "    <li>Here we have use a commonly used activation function ReLU((Rectified Linear Unit)).\n",
    "    </ul>\n",
    "    </font>\n",
    "<font size=\"+1\">\n",
    "</p> This function will be used in forward propagation.</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z is the linear output of all the neuron of a layers stacked together in a matrix.\n",
    "#A is called Post-activation parameter.\n",
    "#both A and Z are of same dimensions.\n",
    "#cache is for saving parameters used for calculating derivatives during backpropagation.\n",
    "#range: (0,INF)\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "#we will use sigmoid for final layer as we want our predictons either A wins or B \n",
    "#range: (0,1)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">\n",
    "            </p>This will be used in back propagation. </p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will be used in back propagation.\n",
    "#dA is the post-gradient parameter,i.e. partial derivation of cost funcion with respect to activation function A.\n",
    "#cache is where we store Z for efficient calculation of gradients during backprop.\n",
    "#this is specifically made for relu activation function, thus the name.\n",
    "#calculation of derivative dZ is activation function dependent. \n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0     \n",
    "    \n",
    "    return dZ\n",
    "\n",
    "#Go through the maths for understanding how dZ = dA * s * (1-s)\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)  \n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">We will do following Tasks</font>\n",
    "<font size=\"+1\">\n",
    "<p>Making functions for the following:</p>\n",
    "<ul>\n",
    "<li>Forward Propagation</li>\n",
    "<li>Computation of Loss</li>\n",
    "<li>Backward Propagatoin</li>\n",
    "    <li> Update Parameters</li>\n",
    "      </ul>\n",
    "<p>Loading data and buliding the complete network.</p>\n",
    "  \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Forward propagation</font>\n",
    "<font size=\"+1\">\n",
    "<ul>\n",
    "<li>Initialization parameters.</li>\n",
    "<li>Linear Forward</li>\n",
    "<li>Linear_Activation Forward</li>\n",
    "<li>Forward_Layer Model</li>\n",
    "    </ul>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Initializing parameters</font>\n",
    "<p>\n",
    "<font size=\"+1\">It is done inorder to create weights and biases of required dimensions for needed calculations \n",
    "    and randomization of weights should be greater than 0, \n",
    "so that our model can learn. \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables are self-explanatory\n",
    "#dimensions_of_layers is python list type for the number of units in different layers.\n",
    "\n",
    "def initializing_parameters(dimensions_of_layers):\n",
    "    \n",
    "\n",
    "    #A seed is an initial value that is fed into a pseudo random number generator \n",
    "    #to start or kick off the process of random number generation.\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #number of layers\n",
    "    N = len(dimensions_of_layers)\n",
    "    \n",
    "    #intializing python dictionary for holding parameters\n",
    "    parameters={}\n",
    "    \n",
    "    for n in range(1,N):\n",
    "        \n",
    "        #randomly initializing weights\n",
    "        #dimensions for weights is (number of units in the present layer, number of units in previous layers)\n",
    "        parameters['W' + str(n)] = np.random.randn(dimensions_of_layers[n], dimensions_of_layers[n - 1]) / np.sqrt(dimensions_of_layers[n-1])\n",
    "        \n",
    "        #initializing biases to zero \n",
    "        #dimensions for weights is (number of units in present layer,1)\n",
    "        parameters['b' + str(n)] = np.zeros((dimensions_of_layers[n], 1))\n",
    "    \n",
    "    #return dictionary of initialized parameters    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p=initializing_parameters([16,20,7,5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Linear Forward</font>\n",
    "<p>$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Linear Activation Forward</font>\n",
    "<p>$$A = RELU(Z) = max(0, Z)$$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A-> activation parameter from previous layer\n",
    "#W-> weight matrix of present layer\n",
    "#b-> biase matrix of present layer\n",
    "#applying ReLU on the linear function calculated above to evolve it from a simple linear regressor\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    #fetching values of Z and cache from linear_forward\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "        \n",
    "    #dictionary for storing W,b,A used during backprop\n",
    "    linear_cache = (A_prev,W,b)\n",
    "   \n",
    "    if activation == \"relu\":     \n",
    "        \n",
    "        #this returns A-> activation parameter and the present value of Z to the activation_cache\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        #this returns A-> activation parameter and the present value of Z to the activation_cache\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    #finaly we save our present layer's W,b,Z and previous layer's A_prev into cache dict for backprop use\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Forward Layer Model </font>\n",
    "<font size=\"+1\"><p><center>Combining all forward functions</center></p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters from initializing_parameters\n",
    "\n",
    "def forward_layer_model(features_matrix, parameters):\n",
    "    \n",
    "    #storage for storing caches of every layer\n",
    "    caches = []\n",
    "    \n",
    "    #initializing A with features_matrix as it is the input layer\n",
    "    A = features_matrix\n",
    "    \n",
    "    #number of layer, as parameters contain both W and b,\n",
    "    N = len(parameters)//2\n",
    "   \n",
    "\n",
    "    #loop for applying above functions in every layer\n",
    "    for n in range(1,N):\n",
    "        \n",
    "        #as the input layer is not included and A=features_matrix will be A^[0]\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(n)], parameters['b' + str(n)], activation='relu')\n",
    "        \n",
    "        #appending caches list\n",
    "        caches.append(cache)\n",
    "   \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(N)], parameters['b' + str(N)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    #here A will be of last layer\n",
    "    return AL, caches\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward_layer_model(data_train_feature,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Computation Of Loss</font>\n",
    "<p>$$Cross Entropy(J)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_calc(A_l_layer, target_matrix):\n",
    "    \n",
    "    #m are number of examples in the data-sets \n",
    "    m=target_matrix.shape[1]\n",
    "    \n",
    "    cost = -(np.sum(np.multiply(target_matrix, np.log(A_l_layer)) + np.multiply(1 - target_matrix, np.log(1 - A_l_layer))))/m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Backward propagation</font>\n",
    "<font size=\"+1\">\n",
    "<ul>\n",
    "<li>Linear Backward</li>\n",
    "<li>Linear_Activation Backward</li>\n",
    "<li>Backward_Layer Model</li>\n",
    "    </ul>\n",
    "    </font>\n",
    "<font size=\"+1\"><p>Backprop is needed inorder to calculate gradients of Loss function with respect to Parameters</p></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Linear Backward</font>\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    #fetching values from cache in linear forward \n",
    "    A_prev, W,b = cache\n",
    "    \n",
    "    #m is the no. of columns in A_prev\n",
    "    m=A_prev.shape[1]\n",
    "    \n",
    "    #gradient of loss function with respect to W\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    \n",
    "    #to preserve dimensions\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)    \n",
    "    \n",
    "    #gradient of loss function with respect to A_prev\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    #returning gradients\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Linear activation Backward</font>\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, caches , activation):\n",
    "       \n",
    "    linear_cache, activation_cache = caches\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Backward Layer Model</font>\n",
    "\n",
    "<font size=\"+1\"><p><center>Combining all backward functions</center></p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_layer_model(A_l_layer, target_matrix , caches):\n",
    "    \n",
    "    #intializing gradients dictionary for each layer\n",
    "    grads={}\n",
    "    \n",
    "    #number of layers\n",
    "    N=len(caches)\n",
    "    \n",
    "    #number of examples\n",
    "    m=A_l_layer.shape[1]\n",
    "    \n",
    "    #making sure both target_matrix and A_last_layer are of shame dimensions\n",
    "\n",
    "    target_matrix= target_matrix.reshape(A_l_layer.shape)\n",
    "    \n",
    "    dAL = - (np.divide(target_matrix, A_l_layer) - np.divide(1 - target_matrix, 1 - A_l_layer))\n",
    "    \n",
    "    #-1 is last dictionary of cache so\n",
    "    current_cache = caches[-1]\n",
    "    \n",
    "    #fetching gradients of output layers into our dictionary\n",
    "    grads[\"dA\" + str(N)], grads[\"dW\" + str(N)], grads[\"db\" + str(N)] = linear_activation_backward(dAL,current_cache,\"sigmoid\")\n",
    "    \n",
    "    #iterating backwards for all hidden layers\n",
    "    for n in reversed(range(N-1)):\n",
    "        \n",
    "        #taking caches from backwards\n",
    "        current_cache = caches[n]\n",
    "        \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(n + 2)], current_cache, activation = \"relu\")\n",
    "        \n",
    "        grads[\"dA\" + str(n + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(n + 1)] = dW_temp\n",
    "        grads[\"db\" + str(n + 1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Update Parameters</font>\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    #number of layers\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    #Update\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Predict Function</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features_matrix, target_matrix, parameters):\n",
    "    \n",
    "    #number of examples\n",
    "    m = features_matrix.shape[1]\n",
    "    \n",
    "    #number of layers\n",
    "    n = len(parameters) // 2 \n",
    "    \n",
    "    #initializing prediction matrix\n",
    "    p = np.zeros((1, m),dtype=int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    probability, caches = forward_layer_model(features_matrix, parameters)\n",
    "\n",
    "\n",
    "    # convert probability to 0/1 predictions\n",
    "    #loop for each example\n",
    "    for i in range(probability.shape[1]):\n",
    "        \n",
    "        if probability[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        \n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: %s\" % str(np.sum(p == target_matrix)/float(m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#loading data which I had pre-processed and cleaned earlier\n",
    "data=pd.read_csv(\"final_csv\", sep=\",\")\n",
    "\n",
    "ind=[]\n",
    "for i in range(len(data[\"result\"])):\n",
    "    if (data[\"result\"][i])==2:\n",
    "            ind.append(i)\n",
    "data.drop(ind, inplace=True)\n",
    "data=data.reset_index(drop=True)\n",
    "            \n",
    "#normalising data columns\n",
    "data['age_A']= data['age_A']/40\n",
    "data['age_B']= data['age_B']/40\n",
    "data['height_A']= data['height_A']/200\n",
    "data['height_B']= data['height_B']/200\n",
    "data['reach_A']= data['reach_A']/200\n",
    "data['reach_B']= data['reach_B']/200\n",
    "data['weight_A']= data['weight_A']/200\n",
    "data['weight_B']= data['weight_B']/200\n",
    "data['won_A']= data['won_A']/100\n",
    "data['won_B']= data['won_B']/100\n",
    "data['lost_A']= data['lost_A']/10\n",
    "data['lost_B']= data['lost_B']/10\n",
    "data['drawn_A']= data['drawn_A']/10\n",
    "data['drawn_B']= data['drawn_B']/10\n",
    "data['kos_A']= data['kos_A']/100\n",
    "data['kos_B']=data['kos_B']/100\n",
    "\n",
    "#taking 1090 win_B test cases\n",
    "ind=[]\n",
    "for i in range(len(data[\"result\"])):\n",
    "    if (data[\"result\"][i])==1:\n",
    "            ind.append(i)\n",
    "\n",
    "data_one= data.iloc[ind]\n",
    "\n",
    "#taking 1090 win_A cases\n",
    "ind=[]\n",
    "for i in range(len(data[\"result\"])):\n",
    "    if (data[\"result\"][i])==0:\n",
    "            ind.append(i)\n",
    "    if len(ind)==1090:\n",
    "        break\n",
    "data_zero= data.iloc[ind]\n",
    "\n",
    "data_one=data_one.append(data_zero)\n",
    "\n",
    "data= data_one\n",
    "data=data.reset_index(drop=True)\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "#dividing data into three groups\n",
    "#training dataframe\n",
    "#validation dataframe for hyperparameters tuning\n",
    "#testing dataframe to test model\n",
    "data_train=data.head(n=1500) #training dataframe\n",
    "data_test=data.iloc[1500:1840] # testing dataframe\n",
    "data_validation=data.iloc[1840:2180] #validation dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making features by converting above dataframes into numpy arrays\n",
    "data_train_feature = data_train.as_matrix(columns=['age_A',\n",
    " 'age_B',\n",
    " 'height_A',\n",
    " 'height_B',\n",
    " 'reach_A',\n",
    " 'reach_B',\n",
    " 'weight_A',\n",
    " 'weight_B',\n",
    " 'won_A',\n",
    " 'won_B',\n",
    " 'lost_A',\n",
    " 'lost_B',\n",
    " 'drawn_A',\n",
    " 'drawn_B',\n",
    " 'kos_A',\n",
    " 'kos_B'])\n",
    "\n",
    "data_train_target = data_train.as_matrix(columns=['result'])\n",
    "\n",
    "data_test_feature = data_test.as_matrix(columns=['age_A',\n",
    " 'age_B',\n",
    " 'height_A',\n",
    " 'height_B',\n",
    " 'reach_A',\n",
    " 'reach_B',\n",
    " 'weight_A',\n",
    " 'weight_B',\n",
    " 'won_A',\n",
    " 'won_B',\n",
    " 'lost_A',\n",
    " 'lost_B',\n",
    " 'drawn_A',\n",
    " 'drawn_B',\n",
    " 'kos_A',\n",
    " 'kos_B'])\n",
    "\n",
    "data_test_target = data_test.as_matrix(columns=['result'])\n",
    "\n",
    "data_validation_feature = data_validation.as_matrix(columns=['age_A',\n",
    " 'age_B',\n",
    " 'height_A',\n",
    " 'height_B',\n",
    " 'reach_A',\n",
    " 'reach_B',\n",
    " 'weight_A',\n",
    " 'weight_B',\n",
    " 'won_A',\n",
    " 'won_B',\n",
    " 'lost_A',\n",
    " 'lost_B',\n",
    " 'drawn_A',\n",
    " 'drawn_B',\n",
    " 'kos_A',\n",
    " 'kos_B'])\n",
    "\n",
    "data_validation_target = data_validation.as_matrix(columns=['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting dimensions\n",
    "#as we have examples as our columns\n",
    "data_train_feature=data_train_feature.reshape((16,1500))\n",
    "data_train_target=data_train_target.reshape((1,1500))\n",
    "\n",
    "data_validation_feature=data_validation_feature.reshape((16,340))\n",
    "data_validation_target=data_validation_target.reshape((1,340))\n",
    "\n",
    "data_test_feature=data_test_feature.reshape((16,340))\n",
    "data_test_target=data_test_target.reshape((1,340))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [16,10,8,16,10,8,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(X, Y, layers_dims, learning_rate=3, num_iterations=3000, print_cost=False): \n",
    "   \n",
    "    costs = []          \n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initializing_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "               \n",
    "        # Forward propagation\n",
    "        AL, caches = forward_layer_model(X, parameters)\n",
    "    \n",
    "        # Compute cost\n",
    "        cost = cost_calc(AL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        grads = backward_layer_model(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "               # Print the cost every 100 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692764\n",
      "Cost after iteration 1000: 0.676189\n",
      "Cost after iteration 2000: 0.660084\n",
      "Cost after iteration 3000: 0.650128\n",
      "Cost after iteration 4000: 0.633951\n",
      "Cost after iteration 5000: 0.616926\n",
      "Cost after iteration 6000: 0.605217\n",
      "Cost after iteration 7000: 0.563988\n",
      "Cost after iteration 8000: 0.554435\n",
      "Cost after iteration 9000: 0.557610\n",
      "Cost after iteration 10000: 0.546272\n",
      "Cost after iteration 11000: 0.510672\n",
      "Cost after iteration 12000: 0.514653\n",
      "Cost after iteration 13000: 0.467705\n",
      "Cost after iteration 14000: 0.461463\n",
      "Cost after iteration 15000: 0.529128\n",
      "Cost after iteration 16000: 0.494729\n",
      "Cost after iteration 17000: 0.427885\n",
      "Cost after iteration 18000: 0.435771\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f102a37b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = NN(data_train_feature, data_train_target, layers_dims, learning_rate=0.07,num_iterations=19000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "p=predict(data_train_feature,data_train_target,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting is about 80%\n",
    "\n",
    "#the model is not yet optimized.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
